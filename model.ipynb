{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n        self.padding_idx = padding_idx\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 2))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            true_dist[:, self.padding_idx] = 0\n            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n            if mask.dim() > 0:\n                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n","metadata":{"id":"dpITQsEyHXCj","execution":{"iopub.status.busy":"2023-09-19T11:00:28.103464Z","iopub.execute_input":"2023-09-19T11:00:28.103833Z","iopub.status.idle":"2023-09-19T11:00:28.114298Z","shell.execute_reply.started":"2023-09-19T11:00:28.103803Z","shell.execute_reply":"2023-09-19T11:00:28.113223Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision import models\nfrom torchvision.models._utils import IntermediateLayerGetter\n\n\nclass Vgg(nn.Module):\n    def __init__(self, ss, ks, hidden, pretrained=True, dropout=0.4):\n        super(Vgg, self).__init__()\n\n        cnn = models.vgg19_bn(pretrained=pretrained)\n\n        pool_idx = 0\n\n        for i, layer in enumerate(cnn.features):\n            if isinstance(layer, torch.nn.MaxPool2d):\n                cnn.features[i] = torch.nn.AvgPool2d(kernel_size=ks[pool_idx], stride=ss[pool_idx], padding=0)\n                pool_idx += 1\n\n        self.features = cnn.features\n        self.dropout = nn.Dropout(dropout)\n        self.last_conv_1x1 = nn.Conv2d(512, hidden, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Shape:\n            - x: (N, C, H, W)\n            - output: (W, N, C)\n        \"\"\"\n\n        conv = self.features(x)\n        conv = self.dropout(conv)\n        conv = self.last_conv_1x1(conv)\n\n#        conv = rearrange(conv, 'b d h w -> b d (w h)')\n        conv = conv.transpose(-1, -2)\n        conv = conv.flatten(2)\n        conv = conv.permute(-1, 0, 1)\n        return conv","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YiIHczejJd4d","outputId":"abe3a9c0-edfe-4497-e5f6-9037693103fd","execution":{"iopub.status.busy":"2023-09-19T11:00:28.116505Z","iopub.execute_input":"2023-09-19T11:00:28.117683Z","iopub.status.idle":"2023-09-19T11:00:28.128370Z","shell.execute_reply.started":"2023-09-19T11:00:28.117647Z","shell.execute_reply":"2023-09-19T11:00:28.127341Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\nimport math\nimport torch\nfrom torch import nn\n\nclass LanguageTransformer(nn.Module):\n    def __init__(self, vocab_size,\n                 d_model, nhead,\n                 num_encoder_layers, num_decoder_layers,\n                 dim_feedforward, max_seq_length,\n                 pos_dropout, trans_dropout):\n        super().__init__()\n\n        self.d_model = d_model\n        self.embed_tgt = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n#        self.learned_pos_enc = LearnedPositionalEncoding(d_model, pos_dropout, max_seq_length)\n\n        self.transformer = nn.Transformer(d_model, nhead,\n                                          num_encoder_layers, num_decoder_layers,\n                                          dim_feedforward, trans_dropout)\n\n        self.fc = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        \"\"\"\n        Shape:\n            - src: (W, N, C)\n            - tgt: (T, N)\n            - src_key_padding_mask: (N, S)\n            - tgt_key_padding_mask: (N, T)\n            - memory_key_padding_mask: (N, S)\n            - output: (N, T, E)\n\n        \"\"\"\n        tgt_mask = self.gen_nopeek_mask(tgt.shape[0]).to(src.device)\n\n        src = self.pos_enc(src*math.sqrt(self.d_model))\n\n        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n\n        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n\n        output = output.transpose(0, 1)\n        return self.fc(output)\n\n    def gen_nopeek_mask(self, length):\n        mask = (torch.triu(torch.ones(length, length)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n\n        return mask\n\n    def forward_encoder(self, src):\n        src = self.pos_enc(src*math.sqrt(self.d_model))\n        memory = self.transformer.encoder(src)\n        return memory\n\n    def forward_decoder(self, tgt, memory):\n        tgt_mask = self.gen_nopeek_mask(tgt.shape[0]).to(tgt.device)\n        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n\n        output = self.transformer.decoder(tgt, memory, tgt_mask=tgt_mask)\n        output = output.transpose(0, 1)\n\n        return self.fc(output), memory\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=100):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n\n        return self.dropout(x)\n\nclass LayerNorm(nn.Module):\n    \"A layernorm module in the TF style (epsilon inside the square root).\"\n    def __init__(self, d_model, variance_epsilon=1e-12):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta  = nn.Parameter(torch.zeros(d_model))\n        self.variance_epsilon = variance_epsilon\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n        return self.gamma * x + self.beta\n","metadata":{"id":"FT2DMEfWHXCj","execution":{"iopub.status.busy":"2023-09-19T11:00:28.129984Z","iopub.execute_input":"2023-09-19T11:00:28.130526Z","iopub.status.idle":"2023-09-19T11:00:28.154357Z","shell.execute_reply.started":"2023-09-19T11:00:28.130486Z","shell.execute_reply":"2023-09-19T11:00:28.153241Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class VietOCR(nn.Module):\n    def __init__(self, vocab_size,\n                 cnn_args,\n                 transformer_args):\n\n        super(VietOCR, self).__init__()\n\n        self.cnn = Vgg(**cnn_args)\n        self.transformer = LanguageTransformer(vocab_size, **transformer_args)\n\n\n    def forward(self, img, tgt_input, tgt_key_padding_mask):\n        \"\"\"\n        Shape:\n            - img: (N, C, H, W)\n            - tgt_input: (T, N)\n            - tgt_key_padding_mask: (N, T)\n            - output: b t v\n        \"\"\"\n        src = self.cnn(img)\n        outputs = self.transformer(src, tgt_input, tgt_key_padding_mask=tgt_key_padding_mask)\n        return outputs","metadata":{"id":"KkZU5SmEHXCk","execution":{"iopub.status.busy":"2023-09-19T11:00:28.157874Z","iopub.execute_input":"2023-09-19T11:00:28.158817Z","iopub.status.idle":"2023-09-19T11:00:28.171409Z","shell.execute_reply.started":"2023-09-19T11:00:28.158779Z","shell.execute_reply":"2023-09-19T11:00:28.170362Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Vocab():\n    def __init__(self, chars):\n        self.pad = 0\n        self.go = 1\n        self.eos = 2\n        self.mask_token = 3\n\n        self.chars = chars\n\n        self.c2i = {c:i+4 for i, c in enumerate(chars)}\n\n        self.i2c = {i+4:c for i, c in enumerate(chars)}\n\n        self.i2c[0] = '<pad>'\n        self.i2c[1] = '<sos>'\n        self.i2c[2] = '<eos>'\n        self.i2c[3] = '*'\n\n    def encode(self, chars):\n        return [self.go] + [self.c2i[c] for c in chars] + [self.eos]\n\n    def decode(self, ids):\n        first = 1 if self.go in ids else 0\n        last = ids.index(self.eos) if self.eos in ids else None\n        sent = ''.join([self.i2c[i] for i in ids[first:last]])\n        return sent\n\n    def __len__(self):\n        return len(self.c2i) + 4\n\n    def batch_decode(self, arr):\n        texts = [self.decode(ids) for ids in arr]\n        return texts\n\n    def __str__(self):\n        return self.chars\n","metadata":{"id":"V5bnG-bPHXCk","execution":{"iopub.status.busy":"2023-09-19T11:00:28.223042Z","iopub.execute_input":"2023-09-19T11:00:28.224340Z","iopub.status.idle":"2023-09-19T11:00:28.236013Z","shell.execute_reply.started":"2023-09-19T11:00:28.224300Z","shell.execute_reply":"2023-09-19T11:00:28.234893Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport math\nfrom PIL import Image\nfrom torch.nn.functional import log_softmax, softmax\n\n\ndef translate(img, model, max_seq_length=128, sos_token=1, eos_token=2):\n    \"data: BxCXHxW\"\n    model.eval()\n    device = img.device\n\n    with torch.no_grad():\n        src = model.cnn(img)\n        memory = model.transformer.forward_encoder(src)\n\n        translated_sentence = [[sos_token]*len(img)]\n\n        max_length = 0\n\n        while max_length <= max_seq_length and not all(np.any(np.asarray(translated_sentence).T==eos_token, axis=1)):\n\n            tgt_inp = torch.LongTensor(translated_sentence).to(device)\n\n#            output = model(img, tgt_inp, tgt_key_padding_mask=None)\n#            output = model.transformer(src, tgt_inp, tgt_key_padding_mask=None)\n            output, memory = model.transformer.forward_decoder(tgt_inp, memory)\n            output = softmax(output, dim=-1)\n            output = output.to('cpu')\n\n            values, indices  = torch.topk(output, 5)\n\n            indices = indices[:, -1, 0]\n            indices = indices.tolist()\n\n            values = values[:, -1, 0]\n            values = values.tolist()\n\n            translated_sentence.append(indices)\n            max_length += 1\n\n            del output\n\n        translated_sentence = np.asarray(translated_sentence).T\n\n\n    return translated_sentence\n\n\ndef build_model(config):\n    vocab = Vocab(config['vocab'])\n    device = config['device']\n\n    model = VietOCR(len(vocab),\n            config['cnn'],\n            config['transformer'])\n\n    model = model.to(device)\n\n    return model, vocab\n\ndef resize(w, h, expected_height, image_min_width, image_max_width):\n    new_w = int(expected_height * float(w) / float(h))\n    round_to = 10\n    new_w = math.ceil(new_w/round_to)*round_to\n    new_w = max(new_w, image_min_width)\n    new_w = min(new_w, image_max_width)\n\n    return new_w, expected_height\n\ndef process_image(image, image_height, image_min_width, image_max_width):\n    img = image.convert('RGB')\n\n    w, h = img.size\n    new_w, image_height = resize(w, h, image_height, image_min_width, image_max_width)\n\n    img = img.resize((new_w, image_height), Image.ANTIALIAS)\n\n    img = np.asarray(img).transpose(2,0, 1)\n    img = img/255\n    return img\n\ndef process_input(image, image_height, image_min_width, image_max_width):\n    img = process_image(image, image_height, image_min_width, image_max_width)\n    img = img[np.newaxis, ...]\n    img = torch.FloatTensor(img)\n    return img\n\n","metadata":{"id":"TbZRdH2VHXCl","execution":{"iopub.status.busy":"2023-09-19T11:00:28.238353Z","iopub.execute_input":"2023-09-19T11:00:28.238983Z","iopub.status.idle":"2023-09-19T11:00:28.255699Z","shell.execute_reply.started":"2023-09-19T11:00:28.238945Z","shell.execute_reply":"2023-09-19T11:00:28.254630Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\n!pip install lmdb\nimport lmdb # install lmdb by \"pip install lmdb\"\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\ndef checkImageIsValid(imageBin):\n    isvalid = True\n    imgH = None\n    imgW = None\n\n    imageBuf = np.fromstring(imageBin, dtype=np.uint8)\n    try:\n        img = cv2.imdecode(imageBuf, cv2.IMREAD_GRAYSCALE)\n\n        imgH, imgW = img.shape[0], img.shape[1]\n        if imgH * imgW == 0:\n            isvalid = False\n    except Exception as e:\n        isvalid = False\n\n    return isvalid, imgH, imgW\n\ndef writeCache(env, cache):\n    with env.begin(write=True) as txn:\n        for k, v in cache.items():\n            txn.put(k.encode(), v)\n\ndef createDataset(outputPath, root_dir, annotation_path):\n    \"\"\"\n    Create LMDB dataset for CRNN training.\n    ARGS:\n        outputPath    : LMDB output path\n        imagePathList : list of image path\n        labelList     : list of corresponding groundtruth texts\n        lexiconList   : (optional) list of lexicon lists\n        checkValid    : if true, check the validity of every image\n    \"\"\"\n\n    annotation_path = os.path.join(root_dir, annotation_path)\n    with open(annotation_path, 'r') as ann_file:\n        lines = ann_file.readlines()\n        annotations = [l.strip().split('\\t') for l in lines]\n\n    nSamples = len(annotations)\n    env = lmdb.open(outputPath, map_size=1099511627776)\n    cache = {}\n    cnt = 0\n    error = 0\n\n    pbar = tqdm(range(nSamples), ncols = 100, desc='Create {}'.format(outputPath))\n    for i in pbar:\n        imageFile, label = annotations[i]\n        imagePath = os.path.join(root_dir, imageFile)\n\n        if not os.path.exists(imagePath):\n            error += 1\n            continue\n\n        with open(imagePath, 'rb') as f:\n            imageBin = f.read()\n        isvalid, imgH, imgW = checkImageIsValid(imageBin)\n\n        if not isvalid:\n            error += 1\n            continue\n\n        imageKey = 'image-%09d' % cnt\n        labelKey = 'label-%09d' % cnt\n        pathKey = 'path-%09d' % cnt\n        dimKey = 'dim-%09d' % cnt\n\n        cache[imageKey] = imageBin\n        cache[labelKey] = label.encode()\n        cache[pathKey] = imageFile.encode()\n        cache[dimKey] = np.array([imgH, imgW], dtype=np.int32).tobytes()\n\n        cnt += 1\n\n        if cnt % 1000 == 0:\n            writeCache(env, cache)\n            cache = {}\n\n    nSamples = cnt-1\n    cache['num-samples'] = str(nSamples).encode()\n    writeCache(env, cache)\n\n    if error > 0:\n        print('Remove {} invalid images'.format(error))\n    print('Created dataset with %d samples' % nSamples)\n    sys.stdout.flush()\n\n#tạo data(train-hw và valid_hw)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSAVDPlXHXCm","outputId":"4611db95-82c2-414b-f0ce-b328a7b960db","execution":{"iopub.status.busy":"2023-09-19T11:00:28.258452Z","iopub.execute_input":"2023-09-19T11:00:28.259183Z","iopub.status.idle":"2023-09-19T11:00:40.640784Z","shell.execute_reply.started":"2023-09-19T11:00:28.259134Z","shell.execute_reply":"2023-09-19T11:00:40.639476Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lmdb in /opt/conda/lib/python3.10/site-packages (1.4.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nimport os\nimport random\nfrom PIL import Image\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom collections import defaultdict\nimport numpy as np\nimport torch\nimport lmdb\nimport six\nimport time\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import Sampler\n\nclass OCRDataset(Dataset):\n    def __init__(self, lmdb_path, root_dir, annotation_path, vocab, image_height=32, image_min_width=32, image_max_width=256, transform=None):\n        self.root_dir = root_dir\n        self.annotation_path = os.path.join(root_dir, annotation_path)\n        self.vocab = vocab\n        self.transform = transform\n\n        self.image_height = image_height\n        self.image_min_width = image_min_width\n        self.image_max_width = image_max_width\n\n        self.lmdb_path =  lmdb_path\n\n        if os.path.isdir(self.lmdb_path):\n            print('{} exists. Remove folder if you want to create new dataset'.format(self.lmdb_path))\n            sys.stdout.flush()\n        else:\n            createDataset(self.lmdb_path, root_dir, annotation_path)\n\n        self.env = lmdb.open(\n            self.lmdb_path,\n            max_readers=8,\n            readonly=True,\n            lock=False,\n            readahead=False,\n            meminit=False)\n        self.txn = self.env.begin(write=False)\n\n        nSamples = int(self.txn.get('num-samples'.encode()))\n        self.nSamples = nSamples\n\n        self.build_cluster_indices()\n\n    def build_cluster_indices(self):\n        self.cluster_indices = defaultdict(list)\n\n        pbar = tqdm(range(self.__len__()),\n                desc='{} build cluster'.format(self.lmdb_path),\n                ncols = 100, position=0, leave=True)\n\n        for i in pbar:\n            bucket = self.get_bucket(i)\n            self.cluster_indices[bucket].append(i)\n\n\n    def get_bucket(self, idx):\n        key = 'dim-%09d'%idx\n\n        dim_img = self.txn.get(key.encode())\n        dim_img = np.fromstring(dim_img, dtype=np.int32)\n        imgH, imgW = dim_img\n\n        new_w, image_height = resize(imgW, imgH, self.image_height, self.image_min_width, self.image_max_width)\n\n        return new_w\n\n    def read_buffer(self, idx):\n        img_file = 'image-%09d'%idx\n        label_file = 'label-%09d'%idx\n        path_file = 'path-%09d'%idx\n\n        imgbuf = self.txn.get(img_file.encode())\n\n        label = self.txn.get(label_file.encode()).decode()\n        img_path = self.txn.get(path_file.encode()).decode()\n\n        buf = six.BytesIO()\n        buf.write(imgbuf)\n        buf.seek(0)\n\n        return buf, label, img_path\n\n    def read_data(self, idx):\n        buf, label, img_path = self.read_buffer(idx)\n\n        img = Image.open(buf).convert('RGB')\n\n        if self.transform:\n            img = self.transform(img)\n\n        img_bw = process_image(img, self.image_height, self.image_min_width, self.image_max_width)\n\n        word = self.vocab.encode(label)\n\n        return img_bw, word, img_path\n\n    def __getitem__(self, idx):\n        img, word, img_path = self.read_data(idx)\n\n        img_path = os.path.join(self.root_dir, img_path)\n\n        sample = {'img': img, 'word': word, 'img_path': img_path}\n\n        return sample\n\n    def __len__(self):\n        return self.nSamples\n\nclass ClusterRandomSampler(Sampler):\n\n    def __init__(self, data_source, batch_size, shuffle=True):\n        self.data_source = data_source\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def flatten_list(self, lst):\n        return [item for sublist in lst for item in sublist]\n\n    def __iter__(self):\n        batch_lists = []\n        for cluster, cluster_indices in self.data_source.cluster_indices.items():\n            if self.shuffle:\n                random.shuffle(cluster_indices)\n\n            batches = [cluster_indices[i:i + self.batch_size] for i in range(0, len(cluster_indices), self.batch_size)]\n            batches = [_ for _ in batches if len(_) == self.batch_size]\n            if self.shuffle:\n                random.shuffle(batches)\n\n            batch_lists.append(batches)\n\n        lst = self.flatten_list(batch_lists)\n        if self.shuffle:\n            random.shuffle(lst)\n\n        lst = self.flatten_list(lst)\n\n        return iter(lst)\n\n    def __len__(self):\n        return len(self.data_source)\n\nclass Collator(object):\n    def __init__(self, masked_language_model=True):\n        self.masked_language_model = masked_language_model\n\n    def __call__(self, batch):\n        filenames = []\n        img = []\n        target_weights = []\n        tgt_input = []\n        max_label_len = max(len(sample['word']) for sample in batch)\n        for sample in batch:\n            img.append(sample['img'])\n            filenames.append(sample['img_path'])\n            label = sample['word']\n            label_len = len(label)\n\n\n            tgt = np.concatenate((\n                label,\n                np.zeros(max_label_len - label_len, dtype=np.int32)))\n            tgt_input.append(tgt)\n\n            one_mask_len = label_len - 1\n\n            target_weights.append(np.concatenate((\n                np.ones(one_mask_len, dtype=np.float32),\n                np.zeros(max_label_len - one_mask_len,dtype=np.float32))))\n\n        img = np.array(img, dtype=np.float32)\n\n\n        tgt_input = np.array(tgt_input, dtype=np.int64).T\n        tgt_output = np.roll(tgt_input, -1, 0).T\n        tgt_output[:, -1]=0\n\n        # random mask token\n        if self.masked_language_model:\n            mask = np.random.random(size=tgt_input.shape) < 0.05\n            mask = mask & (tgt_input != 0) & (tgt_input != 1) & (tgt_input != 2)\n            tgt_input[mask] = 3\n\n        tgt_padding_mask = np.array(target_weights)==0\n\n        rs = {\n            'img': torch.FloatTensor(img),\n            'tgt_input': torch.LongTensor(tgt_input),\n            'tgt_output': torch.LongTensor(tgt_output),\n            'tgt_padding_mask': torch.BoolTensor(tgt_padding_mask),\n            'filenames': filenames\n        }\n\n        return rs\n","metadata":{"id":"Ui7BcTrfHXCm","execution":{"iopub.status.busy":"2023-09-19T11:00:40.644701Z","iopub.execute_input":"2023-09-19T11:00:40.645018Z","iopub.status.idle":"2023-09-19T11:00:40.681518Z","shell.execute_reply.started":"2023-09-19T11:00:40.644988Z","shell.execute_reply":"2023-09-19T11:00:40.680460Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from torch.optim import Adam, SGD, AdamW\nfrom torch import nn\n\n!pip install Levenshtein\nfrom Levenshtein import distance as lev\nimport yaml\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR, OneCycleLR\n\nimport torchvision\n\nfrom PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport time\n\nclass Trainer():\n    def __init__(self, config, pretrained=True, augmentor=None):\n\n        self.config = config\n        self.model, self.vocab = build_model(config)\n\n        self.device = config['device']\n        self.num_iters = config['trainer']['iters']\n\n        self.data_root = config['dataset']['data_root']\n        self.train_annotation = config['dataset']['train_annotation']\n        self.valid_annotation = config['dataset']['valid_annotation']\n        self.dataset_name = config['dataset']['name']\n\n        self.batch_size = config['trainer']['batch_size']\n        self.print_every = config['trainer']['print_every']\n        self.valid_every = config['trainer']['valid_every']\n        \n        self.masked_language_model = config['aug']['masked_language_model']\n\n        self.export_weights = config['trainer']['export']\n\n        \n\n\n        self.iter = 0\n\n        self.optimizer = AdamW(self.model.parameters(), betas=(0.9, 0.98), eps=1e-09)\n        self.scheduler = OneCycleLR(self.optimizer, total_steps=self.num_iters, **config['optimizer'])\n        self.criterion = LabelSmoothingLoss(len(self.vocab), padding_idx=self.vocab.pad, smoothing=0.1)#regularization\n\n\n\n        self.train_gen = self.data_gen('train_{}'.format(self.dataset_name),\n                self.data_root, self.train_annotation, self.masked_language_model)\n        if self.valid_annotation:\n            self.valid_gen = self.data_gen('valid_{}'.format(self.dataset_name),\n                    self.data_root, self.valid_annotation, masked_language_model=False)\n\n        self.train_losses = []\n\n    def train(self):\n        self.save_weights(self.export_weights)\n        total_loss = 0\n\n        total_loader_time = 0\n        total_gpu_time = 0\n        smallestCER = 1e15\n\n        data_iter = iter(self.train_gen)\n        for i in range(self.num_iters):\n            self.iter += 1\n\n            start = time.time()\n\n            try:\n                batch = next(data_iter)\n            except StopIteration:\n                data_iter = iter(self.train_gen)\n                batch = next(data_iter)\n\n            total_loader_time += time.time() - start\n\n            start = time.time()\n            loss = self.step(batch)\n            total_gpu_time += time.time() - start\n\n            total_loss += loss\n\n            if self.iter % self.print_every == 0:\n                info = 'iter: {:06d} - train loss: {:.3f} - lr: {:.2e} - load time: {:.2f} - gpu time: {:.2f}'.format(self.iter,\n                        total_loss/self.print_every, self.optimizer.param_groups[0]['lr'],\n                        total_loader_time, total_gpu_time)\n\n                total_loss = 0\n                total_loader_time = 0\n                total_gpu_time = 0\n                print(info)\n\n            if self.iter % self.valid_every == 0:\n                val_loss = self.validate()\n                totalCER = self.computeTotalCER()\n\n                info = 'iter: {:06d} - valid loss: {:.3f} - totalCER: {:.4f}'.format(self.iter, val_loss, totalCER)\n                print(info)\n\n                if totalCER < smallestCER:#save best acc\n                    self.save_weights(self.export_weights)\n                    smallestCER = totalCER\n\n\n    def validate(self):\n        self.model.eval()\n\n        total_loss = []\n\n        with torch.no_grad():\n            for step, batch in enumerate(self.valid_gen):\n                batch = self.batch_to_device(batch)\n                img, tgt_input, tgt_output, tgt_padding_mask = batch['img'], batch['tgt_input'], batch['tgt_output'], batch['tgt_padding_mask']\n\n                outputs = self.model(img, tgt_input, tgt_padding_mask)\n#                loss = self.criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_output, 'b o -> (b o)'))\n\n                outputs = outputs.flatten(0,1)\n                tgt_output = tgt_output.flatten()\n                loss = self.criterion(outputs, tgt_output)\n\n                total_loss.append(loss.item())\n\n                del outputs\n                del loss\n\n        total_loss = np.mean(total_loss)\n        self.model.train()\n\n        return total_loss\n\n    def predict(self, sample=None):\n        pred_sents = []\n        actual_sents = []\n\n        for batch in  self.valid_gen:\n            batch = self.batch_to_device(batch)\n\n            translated_sentence = translate(batch['img'], self.model)\n\n            pred_sent = self.vocab.batch_decode(translated_sentence.tolist())\n            actual_sent = self.vocab.batch_decode(batch['tgt_output'].tolist())\n\n            pred_sents.extend(pred_sent)\n            actual_sents.extend(actual_sent)\n\n            if sample != None and len(pred_sents) > sample:\n                break\n\n        return pred_sents, actual_sents\n    def predict_batch(self, imgs):\n        self.model.load_state_dict(torch.load(self.export_weights, map_location=torch.device(self.device)))\n        bucket = defaultdict(list)\n        bucket_idx = defaultdict(list)\n        bucket_pred = {}\n\n        sents = [0]*len(imgs)\n\n        for i, img in enumerate(imgs):\n            img = process_input(img, self.config['dataset']['image_height'],\n                self.config['dataset']['image_min_width'], self.config['dataset']['image_max_width'])\n\n            bucket[img.shape[-1]].append(img)\n            bucket_idx[img.shape[-1]].append(i)\n\n\n        for k, batch in bucket.items():\n            batch = torch.cat(batch, 0).to(self.device)\n            s = translate(batch, self.model)\n\n            s = s.tolist()\n            s = self.vocab.batch_decode(s)\n\n            bucket_pred[k] = s\n\n\n        for k in bucket_pred:\n            idx = bucket_idx[k]\n            sent = bucket_pred[k]\n            for i, j in enumerate(idx):\n                sents[j] = sent[i]\n\n        return sents\n\n    def computeTotalCER(self, sample=None):\n        arr1, arr2 = self.predict(sample=sample)\n        num_exam = min(len(arr1), len(arr2))\n        total_loss = 0\n        for i in range(num_exam):\n            total_loss += lev(arr1[i], arr2[i])\n        return total_loss\n\n    def save_weights(self, filename):\n        path, _ = os.path.split(filename)\n        os.makedirs(path, exist_ok=True)\n\n        torch.save(self.model.state_dict(), filename)\n\n    def batch_to_device(self, batch):\n        img = batch['img'].to(self.device, non_blocking=True)\n        tgt_input = batch['tgt_input'].to(self.device, non_blocking=True)\n        tgt_output = batch['tgt_output'].to(self.device, non_blocking=True)\n        tgt_padding_mask = batch['tgt_padding_mask'].to(self.device, non_blocking=True)\n\n        batch = {\n                'img': img, 'tgt_input':tgt_input,\n                'tgt_output':tgt_output, 'tgt_padding_mask':tgt_padding_mask,\n                'filenames': batch['filenames']\n                }\n\n        return batch\n\n    def data_gen(self, lmdb_path, data_root, annotation, masked_language_model=True, transform=None):\n        dataset = OCRDataset(lmdb_path=lmdb_path,\n                root_dir=data_root, annotation_path=annotation,\n                vocab=self.vocab, transform=transform,\n                image_height=self.config['dataset']['image_height'],\n                image_min_width=self.config['dataset']['image_min_width'],\n                image_max_width=self.config['dataset']['image_max_width'])\n\n        sampler = ClusterRandomSampler(dataset, self.batch_size, True)\n        collate_fn = Collator(masked_language_model)\n\n        gen = DataLoader(\n                dataset,\n                batch_size=self.batch_size,\n                sampler=sampler,\n                collate_fn = collate_fn,\n                shuffle=False,\n                drop_last=False,\n                **self.config['dataloader'])\n\n        return gen\n\n\n    def step(self, batch):#update parameter\n        self.model.train()\n\n        batch = self.batch_to_device(batch)\n        img, tgt_input, tgt_output, tgt_padding_mask = batch['img'], batch['tgt_input'], batch['tgt_output'], batch['tgt_padding_mask']\n\n        outputs = self.model(img, tgt_input, tgt_key_padding_mask=tgt_padding_mask)\n#        loss = self.criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_output, 'b o -> (b o)'))\n        outputs = outputs.view(-1, outputs.size(2))#flatten(0, 1)\n        tgt_output = tgt_output.view(-1)#flatten()\n\n        loss = self.criterion(outputs, tgt_output)\n\n        self.optimizer.zero_grad()\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1)\n\n        self.optimizer.step()\n        self.scheduler.step()\n\n        loss_item = loss.item()\n\n        return loss_item\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGasJQokHXCm","outputId":"5aa5a8a9-6899-418b-ba41-69340a350a83","execution":{"iopub.status.busy":"2023-09-19T11:00:40.685217Z","iopub.execute_input":"2023-09-19T11:00:40.685538Z","iopub.status.idle":"2023-09-19T11:00:52.543450Z","shell.execute_reply.started":"2023-09-19T11:00:40.685502Z","shell.execute_reply":"2023-09-19T11:00:52.542241Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Requirement already satisfied: Levenshtein in /opt/conda/lib/python3.10/site-packages (0.21.1)\nRequirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"config = {'vocab': 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ-',\n 'device': 'cuda:0',\n 'seq_modeling': 'transformer',\n 'transformer': {'d_model': 256,\n  'nhead': 8,\n  'num_encoder_layers': 6,\n  'num_decoder_layers': 6,\n  'dim_feedforward': 2048,\n  'max_seq_length': 1024,\n  'pos_dropout': 0.1,\n  'trans_dropout': 0.1},\n 'optimizer': {'max_lr': 0.0003, 'pct_start': 0.1},\n 'trainer': {'batch_size': 64,\n  'print_every': 200,\n  'valid_every': 3000,\n  'iters': 100000,\n  'export': './weights/transformerocr.pth',\n  'checkpoint': './checkpoint/transformerocr_checkpoint.pth',\n  'log': './train.log',\n  'metrics': 10000},\n 'dataset': {'name': 'hw',\n  'data_root': '/kaggle/input/my-data/new_train/new_train/',\n  'train_annotation': 'training_gt.txt',\n  'valid_annotation': 'valid_gt.txt',\n  'image_height': 32,\n  'image_min_width': 32,\n  'image_max_width': 256},\n 'dataloader': {'num_workers': 3, 'pin_memory': True},\n 'aug': { 'masked_language_model': True},\n 'predictor': {'beamsearch': False},\n 'quiet': False,\n 'pretrain': '',\n 'weights': '',\n 'backbone': 'vgg19_bn',\n 'cnn': {'pretrained': True,\n  'ss': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n  'ks': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n  'hidden': 256}}","metadata":{"id":"rWE2LSl3H26F","execution":{"iopub.status.busy":"2023-09-19T11:00:52.545526Z","iopub.execute_input":"2023-09-19T11:00:52.545913Z","iopub.status.idle":"2023-09-19T11:00:52.556035Z","shell.execute_reply.started":"2023-09-19T11:00:52.545877Z","shell.execute_reply":"2023-09-19T11:00:52.555129Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"config","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cAJ1QB9H-oi","outputId":"1ec5e3df-5a01-4cf4-a5e5-49bca39a7f50","execution":{"iopub.status.busy":"2023-09-19T11:00:52.557373Z","iopub.execute_input":"2023-09-19T11:00:52.557691Z","iopub.status.idle":"2023-09-19T11:00:52.578086Z","shell.execute_reply.started":"2023-09-19T11:00:52.557660Z","shell.execute_reply":"2023-09-19T11:00:52.577001Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'vocab': 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ-',\n 'device': 'cuda:0',\n 'seq_modeling': 'transformer',\n 'transformer': {'d_model': 256,\n  'nhead': 8,\n  'num_encoder_layers': 6,\n  'num_decoder_layers': 6,\n  'dim_feedforward': 2048,\n  'max_seq_length': 1024,\n  'pos_dropout': 0.1,\n  'trans_dropout': 0.1},\n 'optimizer': {'max_lr': 0.0003, 'pct_start': 0.1},\n 'trainer': {'batch_size': 64,\n  'print_every': 200,\n  'valid_every': 3000,\n  'iters': 100000,\n  'export': './weights/transformerocr.pth',\n  'checkpoint': './checkpoint/transformerocr_checkpoint.pth',\n  'log': './train.log',\n  'metrics': 10000},\n 'dataset': {'name': 'hw',\n  'data_root': '/kaggle/input/my-data/new_train/new_train/',\n  'train_annotation': 'training_gt.txt',\n  'valid_annotation': 'valid_gt.txt',\n  'image_height': 32,\n  'image_min_width': 32,\n  'image_max_width': 256},\n 'dataloader': {'num_workers': 3, 'pin_memory': True},\n 'aug': {'masked_language_model': True},\n 'predictor': {'beamsearch': False},\n 'quiet': False,\n 'pretrain': '',\n 'weights': '',\n 'backbone': 'vgg19_bn',\n 'cnn': {'pretrained': True,\n  'ss': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n  'ks': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n  'hidden': 256}}"},"metadata":{}}]},{"cell_type":"markdown","source":"mỗi iter lấy 1 batch ra để train\n","metadata":{"id":"jhQoXFJ-NQ-n"}},{"cell_type":"code","source":"trainer = Trainer(config)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXD5WbaENrm5","outputId":"c135e36d-05aa-402e-aadd-c88d6465809b","execution":{"iopub.status.busy":"2023-09-19T11:00:52.581057Z","iopub.execute_input":"2023-09-19T11:00:52.583436Z","iopub.status.idle":"2023-09-19T11:13:42.879600Z","shell.execute_reply.started":"2023-09-19T11:00:52.583411Z","shell.execute_reply":"2023-09-19T11:13:42.878151Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n100%|██████████| 548M/548M [00:05<00:00, 97.7MB/s] \nCreate train_hw:   0%|                                                   | 0/100972 [00:00<?, ?it/s]/tmp/ipykernel_28/2604669442.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n  imageBuf = np.fromstring(imageBin, dtype=np.uint8)\nCreate train_hw: 100%|█████████████████████████████████████| 100972/100972 [12:16<00:00, 137.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Created dataset with 100971 samples\n","output_type":"stream"},{"name":"stderr","text":"train_hw build cluster:   0%|                                            | 0/100971 [00:00<?, ?it/s]/tmp/ipykernel_28/318310328.py:68: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n  dim_img = np.fromstring(dim_img, dtype=np.int32)\ntrain_hw build cluster: 100%|███████████████████████████| 100971/100971 [00:00<00:00, 129630.53it/s]\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nCreate valid_hw: 100%|██████████████████████████████████████████| 2028/2028 [00:20<00:00, 99.21it/s]","output_type":"stream"},{"name":"stdout","text":"Created dataset with 2027 samples\n","output_type":"stream"},{"name":"stderr","text":"\nvalid_hw build cluster: 100%|███████████████████████████████| 2027/2027 [00:00<00:00, 115800.68it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bwvywpZaOIxl","outputId":"423babed-ecf9-49a6-f6b6-32fd92cf5888","execution":{"iopub.status.busy":"2023-09-19T11:13:42.881094Z","iopub.execute_input":"2023-09-19T11:13:42.881472Z","iopub.status.idle":"2023-09-19T11:17:24.496413Z","shell.execute_reply.started":"2023-09-19T11:13:42.881438Z","shell.execute_reply":"2023-09-19T11:17:24.493268Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/4154142708.py:77: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  img = img.resize((new_w, image_height), Image.ANTIALIAS)\n/tmp/ipykernel_28/4154142708.py:77: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  img = img.resize((new_w, image_height), Image.ANTIALIAS)\n/tmp/ipykernel_28/4154142708.py:77: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  img = img.resize((new_w, image_height), Image.ANTIALIAS)\n/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"iter: 000200 - train loss: 2.247 - lr: 1.23e-05 - load time: 0.34 - gpu time: 32.25\niter: 000400 - train loss: 2.027 - lr: 1.31e-05 - load time: 0.13 - gpu time: 24.81\niter: 000600 - train loss: 1.845 - lr: 1.46e-05 - load time: 0.09 - gpu time: 24.10\niter: 000800 - train loss: 1.784 - lr: 1.65e-05 - load time: 0.14 - gpu time: 25.35\niter: 001000 - train loss: 1.660 - lr: 1.90e-05 - load time: 0.14 - gpu time: 24.48\niter: 001200 - train loss: 1.606 - lr: 2.21e-05 - load time: 0.13 - gpu time: 25.27\niter: 001400 - train loss: 1.530 - lr: 2.57e-05 - load time: 0.12 - gpu time: 24.66\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/4154142708.py:77: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  img = img.resize((new_w, image_height), Image.ANTIALIAS)\n/tmp/ipykernel_28/4154142708.py:77: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  img = img.resize((new_w, image_height), Image.ANTIALIAS)\n/tmp/ipykernel_28/4154142708.py:77: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  img = img.resize((new_w, image_height), Image.ANTIALIAS)\n","output_type":"stream"},{"name":"stdout","text":"iter: 001600 - train loss: 1.431 - lr: 2.98e-05 - load time: 0.70 - gpu time: 24.93\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[23], line 83\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m total_loader_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     82\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 83\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m total_gpu_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     86\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n","Cell \u001b[0;32mIn[23], line 247\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_to_device(batch)\n\u001b[1;32m    245\u001b[0m         img, tgt_input, tgt_output, tgt_padding_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_input\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_output\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_padding_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 247\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#        loss = self.criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_output, 'b o -> (b o)'))\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;66;03m#flatten(0, 1)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[18], line 21\u001b[0m, in \u001b[0;36mVietOCR.forward\u001b[0;34m(self, img, tgt_input, tgt_key_padding_mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mShape:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    - img: (N, C, H, W)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    - output: b t v\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(img)\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[17], line 36\u001b[0m, in \u001b[0;36mLanguageTransformer.forward\u001b[0;34m(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, src_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, memory_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Shape:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m        - src: (W, N, C)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_nopeek_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     38\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(src\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[1;32m     40\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tgt(tgt) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n","Cell \u001b[0;32mIn[17], line 49\u001b[0m, in \u001b[0;36mLanguageTransformer.gen_nopeek_mask\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_nopeek_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, length):\n\u001b[0;32m---> 49\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mtriu(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0.0\u001b[39m))\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# config['weights'] = '/kaggle/input/weight/transformerocr (1).pth'\n\n# detector = Predictor(config)\nlink = \"/kaggle/input/my-data/public_test_data/new_public_test/\"\nimgs = []\nnum = 0\nimage_name = os.listdir(link)\nfor i in os.listdir(link):\n      img = link + i\n      img = Image.open(img)\n      imgs.append(img)\n      print(num)\n      num+=1\ns = trainer.predict_batch(imgs)\nwith open('prediction1.txt', 'w') as wf:#vị trí file prediction\n    for i in range(num):\n      wf.write(image_name[i] + \"\\t\" + s[i] + \"\\n\")\n      print(i)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IACu7fNgOPVG","outputId":"9d2935ea-f01c-4024-c4c7-548d8b9880b3","execution":{"iopub.status.busy":"2023-09-19T11:17:27.906293Z","iopub.execute_input":"2023-09-19T11:17:27.906866Z","iopub.status.idle":"2023-09-19T11:17:35.929492Z","shell.execute_reply.started":"2023-09-19T11:17:27.906820Z","shell.execute_reply":"2023-09-19T11:17:35.928094Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(link):\n\u001b[1;32m      9\u001b[0m       img \u001b[38;5;241m=\u001b[39m link \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m---> 10\u001b[0m       img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m       imgs\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     12\u001b[0m       \u001b[38;5;28mprint\u001b[39m(num)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3245\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3242\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3243\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3245\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3247\u001b[0m preinit()\n\u001b[1;32m   3249\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}